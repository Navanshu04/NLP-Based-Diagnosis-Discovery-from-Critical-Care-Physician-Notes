{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "br8FbYjL1R_o"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VMp_xnICkeK",
        "outputId": "f23fb26a-d48d-4d5d-f36d-12bcdc344d4a"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ijRzTCd11Xb2",
        "outputId": "fdba6294-2ee8-48e0-bddd-a103b52d0747"
      },
      "source": [
        "notes = pd.read_csv('C:/Users/Asus/Desktop/notes_physician_texttrim.csv')\n",
        "notes.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUBJECT_ID</th>\n",
              "      <th>HADM_ID</th>\n",
              "      <th>CHARTDATE</th>\n",
              "      <th>CHARTTIME</th>\n",
              "      <th>STORETIME</th>\n",
              "      <th>CATEGORY</th>\n",
              "      <th>DESCRIPTION</th>\n",
              "      <th>ISERROR</th>\n",
              "      <th>TEXT</th>\n",
              "      <th>TEXT_TRIM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>54610</td>\n",
              "      <td>100003.0</td>\n",
              "      <td>2150-04-17</td>\n",
              "      <td>2150-04-17 20:48:00</td>\n",
              "      <td>2150-04-17 20:48:38</td>\n",
              "      <td>Physician</td>\n",
              "      <td>Physician Attending Admission Note - MICU</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Chief Complaint:  GIB/HOTN\\n   I saw and exami...</td>\n",
              "      <td>Assessment and Plan\\n   HOTN:   [**Month (only...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>54610</td>\n",
              "      <td>100003.0</td>\n",
              "      <td>2150-04-18</td>\n",
              "      <td>2150-04-18 02:47:00</td>\n",
              "      <td>2150-04-18 14:53:40</td>\n",
              "      <td>Physician</td>\n",
              "      <td>Physician Resident Admission Note</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Chief Complaint:  coffee ground emesis, light ...</td>\n",
              "      <td>Assessment and Plan\\n   Mr. [**Known lastname ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>54610</td>\n",
              "      <td>100003.0</td>\n",
              "      <td>2150-04-18</td>\n",
              "      <td>2150-04-18 08:41:00</td>\n",
              "      <td>2150-04-18 08:42:05</td>\n",
              "      <td>Physician</td>\n",
              "      <td>Physician Attending Progress Note</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Chief Complaint:\\n   I saw and examined the pa...</td>\n",
              "      <td>Assessment and Plan\\n   59 yo M with HCV cirho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>54610</td>\n",
              "      <td>100003.0</td>\n",
              "      <td>2150-04-19</td>\n",
              "      <td>2150-04-19 06:28:00</td>\n",
              "      <td>2150-04-19 09:53:50</td>\n",
              "      <td>Physician</td>\n",
              "      <td>Physician Resident Progress Note</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Chief Complaint:\\n   24 Hour Events:\\n CALLED ...</td>\n",
              "      <td>Assessment and Plan\\n GASTROINTESTINAL BLEED, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>54610</td>\n",
              "      <td>100003.0</td>\n",
              "      <td>2150-04-19</td>\n",
              "      <td>2150-04-19 09:54:00</td>\n",
              "      <td>2150-04-19 09:54:09</td>\n",
              "      <td>Physician</td>\n",
              "      <td>Physician Attending Progress Note</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Chief Complaint:\\n   HPI:\\n   24 Hour Events:\\...</td>\n",
              "      <td>Assessment and Plan\\n   59 yo M HCV cirrhosis ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   SUBJECT_ID  ...                                          TEXT_TRIM\n",
              "0       54610  ...  Assessment and Plan\\n   HOTN:   [**Month (only...\n",
              "1       54610  ...  Assessment and Plan\\n   Mr. [**Known lastname ...\n",
              "2       54610  ...  Assessment and Plan\\n   59 yo M with HCV cirho...\n",
              "3       54610  ...  Assessment and Plan\\n GASTROINTESTINAL BLEED, ...\n",
              "4       54610  ...  Assessment and Plan\\n   59 yo M HCV cirrhosis ...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv4CXhBa2ekA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "132146dd-6224-4983-bd73-69e927561dc7"
      },
      "source": [
        "trim_text = notes[\"TEXT_TRIM\"]\n",
        "# tokens = nltk.word_tokenize(trim_text.iloc[0])\n",
        "# print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Assessment', 'and', 'Plan', 'HOTN', ':', '[', '**Month', '(', 'only', ')', '11**', ']', 'be', 'due', 'to', 'GIB', 'as', 'Hct', 'is', 'falling', ',', 'but', 'however', 'he', 'has', 'had', 'no', 'melena', 'which', 'would', 'be', 'unusual', '.', '[', '**Month', '(', 'only', ')', '11**', ']', 'also', 'be', 'septic', ',', 'although', 'source', 'is', 'unclear', '.', '[', '**Name2', '(', 'NI', ')', '368**', ']', 'not', 'had', 'CXR', 'or', 'U/A', 'yet', '.', 'Will', 'empirically', 'treat', 'with', 'broad', 'spectrum', 'antibiotics', '.', 'Infectious', 'w/u', '.', 'Continue', 'agressive', 'IVF', 'and', 'blood', 'transfusions', 'to', 'Hct', '>', '27', '.', 'Check', 'CVP', 'and', 'keep', '>', '10', 'once', 'Central', 'line', 'is', 'cleared', 'and', 'usable', '.', 'Cirrhosis', ':', 'Would', 'need', 'SBP', 'prophylaxis', ',', 'but', 'should', 'be', 'covered', 'with', 'antibiotics', '.', 'GIB', ':', 'Likely', 'variceal', 'bleed', ',', 'but', 'unclear', 'severity', ',', 'but', 'could', 'explain', 'entire', 'picture', 'in', 'which', 'case', 'he', 'might', 'need', 'EGD', '.', 'Will', 'try', 'again', 'to', 'place', 'an', 'NGT', '.', 'ICU', 'Care', 'Nutrition', ':', 'Glycemic', 'Control', ':', 'Lines', '/', 'Intubation', ':', '18', 'Gauge', '-', '[', '**2150-4-17**', ']', '05:56', 'PM', '16', 'Gauge', '-', '[', '**2150-4-17**', ']', '06:05', 'PM', 'Comments', ':', 'Prophylaxis', ':', 'DVT', ':', 'Stress', 'ulcer', ':', 'VAP', ':', 'Comments', ':', 'Communication', ':', 'Comments', ':', 'Code', 'status', ':', 'Full', 'code', 'Disposition', ':', 'ICU', 'Total', 'time', 'spent', ':', '45', 'minutes', 'Patient', 'is', 'critically', 'ill']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1s7FGmu92JK"
      },
      "source": [
        "import spacy\n",
        "\n",
        "class SWrapper:\n",
        "\t'''\n",
        "\tA class to encapsulate all functionalities that can be exploited from spacy linguistics\n",
        "\t'''\n",
        "\tdef __init__(self, inp_sent='', disable=list()):\n",
        "\t\t'''\n",
        "\t\tInitializing inp sentence inside constructor\n",
        "\t\t'''\n",
        "\n",
        "\t\t# Loading model and processing sentence\n",
        "\t\tself.en_nlp = spacy.load('en', disable=disable)\n",
        "\t\tself.set_sent(inp_sent)\n",
        "\n",
        "\tdef __str__(self):\n",
        "\t\t'''\n",
        "\t\tprints this object\n",
        "\t\t'''\n",
        "\t\treturn self._inp_sent\n",
        "\n",
        "\tdef get_sent(self):\n",
        "\t\t'''\n",
        "\t\tget class property sentence back\n",
        "\t\t'''\n",
        "\t\treturn self._inp_sent\n",
        "\n",
        "\tdef set_sent(self, inp_sent):\n",
        "\t\t'''\n",
        "\t\tset class property sentence\n",
        "\t\t'''\n",
        "\t\tself._inp_sent = inp_sent\n",
        "\t\tself.doc = self.en_nlp(inp_sent)\n",
        "\n",
        "\tdef get_pos(self):\n",
        "\t\t'''\n",
        "\t\treturns superficial pos tags as [[word, tag]...]\n",
        "\t\t'''\n",
        "\t\tpos = list()\n",
        "\t\tfor token in self.doc:\n",
        "\t\t\ttemp = list()\n",
        "\t\t\ttemp.append(token.text)\n",
        "\t\t\ttemp.append(token.pos_)\n",
        "\t\t\tpos.append(temp)\n",
        "\n",
        "\t\treturn pos\n",
        "\n",
        "\tdef get_lemmas(self):\n",
        "\t\t'''\n",
        "\t\treturns a list of tokens in the sentence in their basic form\n",
        "\t\t'''\n",
        "\t\tlemmas = list()\n",
        "\t\tfor token in self.doc:\n",
        "\t\t\ttemp = list()\n",
        "\t\t\ttemp.append(token.text)\n",
        "\t\t\ttemp.append(token.lemma_)\n",
        "\t\t\tlemmas.append(temp)\n",
        "\n",
        "\t\treturn lemmas\n",
        "\n",
        "\tdef get_entities(self):\n",
        "\t\t'''\n",
        "\t\treturns list of tags with thier entity labesl as [[word, label]...]\n",
        "\t\t'''\n",
        "\t\tlabels = list()\n",
        "\t\tfor token in self.doc.ents:\n",
        "\t\t\ttemp = list()\n",
        "\t\t\ttemp.append(token.text)\n",
        "\t\t\ttemp.append(token.label_)\n",
        "\t\t\tlabels.append(temp)\n",
        "\n",
        "\t\treturn labels\n",
        "\n",
        "\tdef get_noun_chunks(self):\n",
        "\t\t'''\n",
        "\t\treturns all noun phrases\n",
        "\t\t'''\n",
        "\t\treturn list(self.doc.noun_chunks)\n",
        "\n",
        "\tdef get_some_tags(self, patterns=list()):\n",
        "\t\t'''\n",
        "\t\treturns all tokens with tags recognized\n",
        "\t\t'''\n",
        "\t\ttokens = list()\n",
        "\t\ttags = self.get_tags()\n",
        "\n",
        "\t\tif type(patterns) != list:\n",
        "\t\t\traise Exception('Please send pos tag patterns in a list !!')\n",
        "\n",
        "\t\t# Checking each element of patterns against tags\n",
        "\t\tfor tag in tags:\n",
        "\t\t\tif any(pattern in tag[1] for pattern in patterns):\n",
        "\t\t\t\ttokens.append(tag[0])\n",
        "\n",
        "\t\treturn tokens\n",
        "\n",
        "\tdef get_tags(self):\n",
        "\t\t'''\n",
        "\t\treturns detailed pos tags as [[word, tag]...]\n",
        "\t\t'''\n",
        "\t\ttag = list()\n",
        "\t\tfor token in self.doc:\n",
        "\t\t\ttemp = list()\n",
        "\t\t\ttemp.append(token.text)\n",
        "\t\t\ttemp.append(token.tag_)\n",
        "\t\t\ttag.append(temp)\n",
        "\n",
        "\t\treturn tag\n",
        "\n",
        "\tdef get_dependency(self):\n",
        "\t\t'''\n",
        "\t\tReturns dependency in format [[relation, governor, dependent]...]\n",
        "\t\t'''\n",
        "\t\tdependencies = list()\n",
        "\t\tfor token in self.doc:\n",
        "\t\t\ttemp = list()\n",
        "\t\t\ttemp.append(token.dep_)\t# adding relation\n",
        "\t\t\ttemp.append(token.head.text) # adding governor/head\n",
        "\t\t\ttemp.append(token.text) # adding dependent\n",
        "\t\t\tdependencies.append(temp)\n",
        "\n",
        "\t\treturn dependencies\n",
        "\n",
        "\tdef is_alpha(self):\n",
        "\t\t'''\n",
        "\t\treturns if word is alpha numeric or not [[word, is_alpha]...]\n",
        "\t\t'''\n",
        "\t\tis_alpha = list()\n",
        "\t\tfor token in self.doc:\n",
        "\t\t\ttemp = list()\n",
        "\t\t\ttemp.append(token.text)\n",
        "\t\t\ttemp.append(token.is_alpha)\n",
        "\t\t\tis_alpha.append(temp)\n",
        "\n",
        "\t\treturn is_alpha\n",
        "\n",
        "\tdef is_stop(self):\n",
        "\t\t'''\n",
        "\t\treturns if word is stop word or not [[word, is_stop]...]\n",
        "\t\t'''\n",
        "\t\tis_stop = list()\n",
        "\t\tfor token in self.doc:\n",
        "\t\t\ttemp = list()\n",
        "\t\t\ttemp.append(token.text)\n",
        "\t\t\ttemp.append(token.is_stop)\n",
        "\t\t\tis_stop.append(temp)\n",
        "\n",
        "\t\treturn is_stop\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "# \tinp_sent = input('Enter Input Sentence for Dependency Parsing : \\n >> ')\n",
        "\n",
        "\t# spacy_obj = SWrapper(inp_sent)\n",
        "\t# print (spacy_obj)\n",
        "\t# print (spacy_obj.get_dependency())\n",
        "\t# print (spacy_obj.get_entities())\n",
        "\t# print (spacy_obj.get_noun_chunks())\n",
        "\t# print (spacy_obj.get_some_tags(patterns=['NN']))\n",
        "\t# print (spacy_obj.get_pos())\n",
        "\t# print (spacy_obj.get_tags())\n",
        "\t# print (spacy_obj.get_lemmas())\n",
        "\t# print (spacy_obj.is_alpha())\n",
        "\t# print (spacy_obj.is_stop())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANmZnPqzKuqc"
      },
      "source": [
        "def get_noun_chunks(statement):\n",
        "  spacy_obj = SWrapper(statement)\n",
        "  return spacy_obj.get_noun_chunks()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZJBFhuFMZDh"
      },
      "source": [
        "new_series = trim_text.apply(get_noun_chunks)\n",
        "new_series"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}